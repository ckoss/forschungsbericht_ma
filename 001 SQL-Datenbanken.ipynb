{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Datenüberblick und SQL-Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autor: Christian Koss - ckoss@uni-bremen.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System-Information: \tLinux-4.4.0-130-generic-x86_64-with-Ubuntu-16.04-xenial\n",
      "Python-Version: \t3.5.2\n"
     ]
    }
   ],
   "source": [
    "# Verwendete Hard-/Software\n",
    "import sys\n",
    "from platform import platform\n",
    "print(\"System-Information: \\t\"+platform())\n",
    "print(\"Python-Version: \\t\"+sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas-Version: \t0.22.0\n",
      "SQLite3-Version: \t3.11.0\n",
      "xml.etree.ET-Version: \t1.3.0\n",
      "RegEx-Version: \t\t2.2.1\n"
     ]
    }
   ],
   "source": [
    "# Verwendete externe Packages\n",
    "import pandas as pd\n",
    "print(\"Pandas-Version: \\t\"+pd.__version__)\n",
    "import sqlite3\n",
    "print(\"SQLite3-Version: \\t\"+sqlite3.sqlite_version)\n",
    "import xml.etree.ElementTree as ET\n",
    "print(\"xml.etree.ET-Version: \\t\"+ET.VERSION)\n",
    "import re\n",
    "print(\"RegEx-Version: \\t\\t\"+re.__version__)\n",
    "import Levenshtein #0.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazon', 'data.db', 'wikipedia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datensätze\n",
    "import os\n",
    "data_path=\"data\"\n",
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon (Webis-CLS-10)\n",
    "\n",
    "Original-Quelle der Daten:<br/>\n",
    "[amazon.de](http://amazon.de)\n",
    "\n",
    "Datenerhebung: <br/>\n",
    "Prettenhofer, P., & Stein, B. (2010). Cross-language text classification using structural correspondence learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, Association for Computational Linguistics, S. 1118-1127\n",
    "<br/><br/>\n",
    "[Datasets](https://www.uni-weimar.de/de/medien/professuren/medieninformatik/webis/data/webis-cls-10/)<br/>\n",
    "[Publikation](https://aclanthology.info/pdf/P/P10/P10-1114.pdf)<br/>\n",
    "[Read Me](https://www.uni-weimar.de/medien/webis/corpora/corpus-webis-cls-10/cls-acl10-unprocessed-README.txt)<br/>\n",
    "<br/>\n",
    "(Zugriff 04.04.2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazon.db', 'music', 'books', 'dvd']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_name=\"amazon\"\n",
    "os.listdir(data_path+'/'+data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "music: \t60392\n",
      "books: \t165470\n",
      "dvd: \t91516\n"
     ]
    }
   ],
   "source": [
    "# XML -> Pandas\n",
    "paths = [data_path+\"/\"+data_name+\"/\"+path+\"/unlabeled.review\" for path in [\"music\",\"books\",\"dvd\"]]\n",
    "music_path, books_path, dvd_path = paths\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "dbs={}\n",
    "for path in paths:\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    db = path.split('/')[2]\n",
    "    dbs[db]=pd.DataFrame([[i]+[child.text for child in root[i]] for i in range(len(root))], columns=['ID']+[child.tag for child in root[0]])\n",
    "    print(db+': \\t'+str(len(root.findall(\"./item\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>realname</th>\n",
       "      <th>asin</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>date</th>\n",
       "      <th>location</th>\n",
       "      <th>helpfulness_votes</th>\n",
       "      <th>reviewer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>DVD &amp;amp; Blu-ray</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>B002GH1M5W</td>\n",
       "      <td>http://www.amazon.de/product-reviews/B002GH1M5W/</td>\n",
       "      <td>X MEN ORIGINS : WOLVERINE präsentiert uns die ...</td>\n",
       "      <td>X-Men Origins: Wolverine (Extended Version ink...</td>\n",
       "      <td>\"...da ist eben ein nackter Mann in unsere Sch...</td>\n",
       "      <td>15. Oktober 2009</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Caligula</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>DVD &amp;amp; Blu-ray</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>B001G7K8QW</td>\n",
       "      <td>http://www.amazon.de/product-reviews/B001G7K8QW/</td>\n",
       "      <td>Komme gerade aus dem Kino. Großartiger Film.\\n...</td>\n",
       "      <td>Willkommen bei den Sch'tis</td>\n",
       "      <td>Nicht nur für Blödbommel und Zipfel, Häää!</td>\n",
       "      <td>2. Januar 2009</td>\n",
       "      <td>Dortmund, Westfalen</td>\n",
       "      <td>None</td>\n",
       "      <td>Pixie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>DVD &amp;amp; Blu-ray</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>B002ACP13M</td>\n",
       "      <td>http://www.amazon.de/product-reviews/B002ACP13M/</td>\n",
       "      <td>Für mich und meine Lebensgefährtin war der Fil...</td>\n",
       "      <td>Illuminati</td>\n",
       "      <td>Greift lieber zum Buch oder zum Hörbuch!</td>\n",
       "      <td>25. Juli 2009</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Curtis Newton \"curtisnewtoncf\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID           category rating realname        asin  \\\n",
       "0   0  DVD &amp; Blu-ray    2.0    False  B002GH1M5W   \n",
       "1   1  DVD &amp; Blu-ray    5.0    False  B001G7K8QW   \n",
       "2   2  DVD &amp; Blu-ray    2.0    False  B002ACP13M   \n",
       "\n",
       "                                                url  \\\n",
       "0  http://www.amazon.de/product-reviews/B002GH1M5W/   \n",
       "1  http://www.amazon.de/product-reviews/B001G7K8QW/   \n",
       "2  http://www.amazon.de/product-reviews/B002ACP13M/   \n",
       "\n",
       "                                                text  \\\n",
       "0  X MEN ORIGINS : WOLVERINE präsentiert uns die ...   \n",
       "1  Komme gerade aus dem Kino. Großartiger Film.\\n...   \n",
       "2  Für mich und meine Lebensgefährtin war der Fil...   \n",
       "\n",
       "                                               title  \\\n",
       "0  X-Men Origins: Wolverine (Extended Version ink...   \n",
       "1                         Willkommen bei den Sch'tis   \n",
       "2                                         Illuminati   \n",
       "\n",
       "                                             summary              date  \\\n",
       "0  \"...da ist eben ein nackter Mann in unsere Sch...  15. Oktober 2009   \n",
       "1         Nicht nur für Blödbommel und Zipfel, Häää!    2. Januar 2009   \n",
       "2           Greift lieber zum Buch oder zum Hörbuch!     25. Juli 2009   \n",
       "\n",
       "              location helpfulness_votes                        reviewer  \n",
       "0                 None              None                        Caligula  \n",
       "1  Dortmund, Westfalen              None                           Pixie  \n",
       "2                 None              None  Curtis Newton \"curtisnewtoncf\"  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbs['dvd'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas -> SQL\n",
    "\n",
    "amazon_sql = data_path+'/'+data_name+'/amazon.db'\n",
    "for db, df in dbs.items():   \n",
    "    conn = sqlite3.connect(amazon_sql)\n",
    "    cur = conn.cursor()                                 \n",
    "\n",
    "    wildcards = ','.join(['?'] * len(df.columns))              \n",
    "\n",
    "    cur.execute(\"drop table if exists %s\" % db)\n",
    "\n",
    "    col_str = '\"' + '\",\"'.join(df.columns) + '\"'\n",
    "    cur.execute(\"create table %s (%s)\" % (db, col_str))\n",
    "\n",
    "    cur.executemany(\"insert into %s values(%s)\" % (db, wildcards), [tuple(x) for x in df.values])\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia\n",
    "\n",
    "Übersicht: [https://dumps.wikimedia.org/dewiki/20180820/](https://dumps.wikimedia.org/dewiki/20180820/)\n",
    "\n",
    "Hinweis: 2018-08-22 07:44:12 done Recombine all pages, current versions only.\n",
    "\n",
    "Download:   [dewiki-20180820-pages-meta-current.xml.bz2](https://dumps.wikimedia.org/dewiki/20180820/dewiki-20180820-pages-meta-current.xml.bz2) 30.6 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dewiki-20180820-pages-meta-current.xml']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_name=\"wikipedia\"\n",
    "\n",
    "os.listdir(data_path+'/'+data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# wiki XML-articles\n",
    "#==============================================================================\n",
    "# Data-Structure\n",
    "# https://www.mediawiki.org/wiki/Manual:Namespace\n",
    "# https://www.mediawiki.org/wiki/Manual:Revision_table\n",
    "#==============================================================================\n",
    "# <page>\n",
    "#     <title></title>                       # Titel der Seite\n",
    "#     <redirect title />                    # Weitergeleitete Titel\n",
    "#     <ns></ns>                             # Namespace: Seitentyp (Artikel/Nutzer/Hilfe/Medien/...)\n",
    "#     <id></id>                             # ID des Artikels\n",
    "#     <revision>                            # Meta-Daten der Editierung\n",
    "#         <id></id>                             # ID der aktuellen Version\n",
    "#         <parentid></parentid>                 # ID der vorherigen Version\n",
    "#         <timestamp></timestamp>               # Zeit der Editierung \n",
    "#         <contributor>                         # Meta-Daten des Erstellers\n",
    "#             <username></username>                 # User-Name\n",
    "#             <id></id>                             # User-ID\n",
    "#             <ip></ip>                             # User-IP\n",
    "#         </contributor>\n",
    "#         <minor />                         # Kennzeichnet minimale Editierungen\n",
    "#         <comment></comment>               # Kommentar des Erstellers\n",
    "#         <model></model>                   # Model des Artikels (wikitext/JS/CSS/...)\n",
    "#         <format></format>                 # Format des Artikels (Wiki/HTML/PHP/...)\n",
    "#         <text>                            # Inhalt/Text des Artikels (https://www.mediawiki.org/wiki/Help:Formatting)\n",
    "#         </text>\n",
    "#         <sha1></sha1>                     # Verschlüsselter Hash\n",
    "#     </revision>\n",
    "# </page>\n",
    "#==============================================================================\n",
    "# Example: https://www.mediawiki.org/wiki/Help:Export#Example\n",
    "#=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages: 6203084\n"
     ]
    }
   ],
   "source": [
    "# XML -> SQL\n",
    "# 6,2 Millionen Seiten\n",
    "\n",
    "# Parse Wikipeda\n",
    "wikipedia_sql = data_path+'/'+data_name+'/wikipedia.db'\n",
    "\n",
    "ns = 'http://www.mediawiki.org/xml/export-0.10/'\n",
    "db = 'wikipages'\n",
    "\n",
    "collector=[]\n",
    "memory=100000 #100.000 = bis zu 0.6GB RAM\n",
    "status=True\n",
    "count=0\n",
    "\n",
    "conn=sqlite3.connect(wikipedia_sql)\n",
    "cur = conn.cursor()  \n",
    "var_format=[('ID','INT'),('title','TEXT'),('text','TEXT'),('infobox','TEXT')]\n",
    "\n",
    "cur.execute(\"drop table if exists %s\" % db)\n",
    "conn.commit()\n",
    "\n",
    "var=[x for (x,y) in var_format]\n",
    "create='CREATE TABLE IF NOT EXISTS '+db+'('+', '.join(var)+')'    \n",
    "wildcards = ','.join(['?'] * len(var))  \n",
    "cur.execute(create) \n",
    "conn.commit()\n",
    "\n",
    "for event, elem in ET.iterparse('data/wikipedia/dewiki-20180820-pages-meta-current.xml'):\n",
    "    if elem.tag=='{%s}page' % ns:\n",
    "        ID=elem.find('{%s}revision' % ns).find('{%s}id' % ns).text\n",
    "        title=elem.find('{%s}title' % ns).text\n",
    "        text=elem.find('{%s}revision' % ns).find('{%s}text' % ns).text\n",
    "        \n",
    "        try:\n",
    "            infobox=re.search('\\{\\{Infobox([^|<}]+)',text).group(1).split('\\n')[0].strip()\n",
    "        except:\n",
    "            infobox=\"None\"\n",
    "            \n",
    "        collector.append((ID,title,text, infobox))\n",
    "        elem.clear() #VERY IMPORTANT TO FREE MEMORY\n",
    "        status=True\n",
    "    if len(collector)>memory:\n",
    "        cur.executemany(\"insert into %s values(%s)\" % (db, wildcards), collector)\n",
    "        conn.commit()\n",
    "        count+=len(collector)\n",
    "        collector=[]\n",
    "        status=False\n",
    "        \n",
    "if status==True:\n",
    "    cur.executemany(\"insert into %s values(%s)\" % (db, wildcards), collector)\n",
    "    conn.commit()\n",
    "    count+=len(collector)\n",
    "    #collector=[]\n",
    "conn.close()\n",
    "\n",
    "print(\"Pages: \"+str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>infobox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>179556416</td>\n",
       "      <td>Alan Smithee</td>\n",
       "      <td>'''Alan Smithee''' steht als [[Pseudonym]] für...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175467883</td>\n",
       "      <td>Actinium</td>\n",
       "      <td>{{Infobox Chemisches Element\\n&lt;!--- Periodensy...</td>\n",
       "      <td>Chemisches Element</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>179845113</td>\n",
       "      <td>Ang Lee</td>\n",
       "      <td>[[Datei:Ang Lee - 66eme Festival de Venise (Mo...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>168173779</td>\n",
       "      <td>Anschluss (Soziologie)</td>\n",
       "      <td>'''Anschluss''' ist in der [[Soziologie]] ein ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38028819</td>\n",
       "      <td>Anschlussfähigkeit</td>\n",
       "      <td>#REDIRECT [[Anschluss (Soziologie)]]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>179064573</td>\n",
       "      <td>Aussagenlogik</td>\n",
       "      <td>Die '''Aussagenlogik''' ist ein Teilgebiet der...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6746470</td>\n",
       "      <td>Autopoiese</td>\n",
       "      <td>#REDIRECT [[Autopoiesis]]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>153932060</td>\n",
       "      <td>A.A.</td>\n",
       "      <td>#redirect [[AA]]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>180174590</td>\n",
       "      <td>Liste von Autoren/A</td>\n",
       "      <td>__NOTOC__\\n\\n{{SubTOC|Titel=Liste von Autoren|...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>178067098</td>\n",
       "      <td>Liste von Autoren/H</td>\n",
       "      <td>__NOTOC__\\n\\n{{SubTOC|Titel=Liste von Autoren|...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                   title  \\\n",
       "0  179556416            Alan Smithee   \n",
       "1  175467883                Actinium   \n",
       "2  179845113                 Ang Lee   \n",
       "3  168173779  Anschluss (Soziologie)   \n",
       "4   38028819      Anschlussfähigkeit   \n",
       "5  179064573           Aussagenlogik   \n",
       "6    6746470              Autopoiese   \n",
       "7  153932060                    A.A.   \n",
       "8  180174590     Liste von Autoren/A   \n",
       "9  178067098     Liste von Autoren/H   \n",
       "\n",
       "                                                text             infobox  \n",
       "0  '''Alan Smithee''' steht als [[Pseudonym]] für...                None  \n",
       "1  {{Infobox Chemisches Element\\n<!--- Periodensy...  Chemisches Element  \n",
       "2  [[Datei:Ang Lee - 66eme Festival de Venise (Mo...                None  \n",
       "3  '''Anschluss''' ist in der [[Soziologie]] ein ...                None  \n",
       "4               #REDIRECT [[Anschluss (Soziologie)]]                None  \n",
       "5  Die '''Aussagenlogik''' ist ein Teilgebiet der...                None  \n",
       "6                          #REDIRECT [[Autopoiesis]]                None  \n",
       "7                                   #redirect [[AA]]                None  \n",
       "8  __NOTOC__\\n\\n{{SubTOC|Titel=Liste von Autoren|...                None  \n",
       "9  __NOTOC__\\n\\n{{SubTOC|Titel=Liste von Autoren|...                None  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_sql='data/wikipedia/wikipedia.db'\n",
    "conn = sqlite3.connect(wikipedia_sql)\n",
    "cur = conn.cursor() \n",
    "\n",
    "cur.execute('SELECT * FROM wikipages') \n",
    "data=[]\n",
    "count=0\n",
    "for text in cur:\n",
    "    count+=1\n",
    "    if count>10:\n",
    "        break;\n",
    "    data.append(text)\n",
    "conn.close()\n",
    "pd.DataFrame(data, columns=['ID','title','text','infobox'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiktionary\n",
    "\n",
    "Übersicht: [https://dumps.wikimedia.org/dewiktionary/20180901/](https://dumps.wikimedia.org/dewiktionary/20180901/)\n",
    "\n",
    "Hinweis: 2018-09-02 19:23:57 done Articles, templates, media/file descriptions, and primary meta-pages\n",
    "\n",
    "Download: [dewiktionary-20180901-pages-articles-multistream.xml.bz2](https://dumps.wikimedia.org/dewiktionary/20180901/dewiktionary-20180901-pages-articles-multistream.xml.bz2) 153.6 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML -> SQL\n",
    "# 625617 n-gram\n",
    "\n",
    "# Parse Wiktionary\n",
    "wiktionary_sql = 'data/wiktionary/wiktionary.db'\n",
    "conn=sqlite3.connect(wiktionary_sql)\n",
    "cur = conn.cursor()\n",
    "cur.execute('SELECT title, text FROM wikipages')\n",
    "\n",
    "bad_pages=['Hilfe:',\n",
    "            'Kategorie:',\n",
    "            'Modul:',\n",
    "            'Reim:',\n",
    "            'Verzeichnis:',\n",
    "            'Vorlage:',\n",
    "            'Flexion:',\n",
    "            'Wiktionary:']\n",
    "\n",
    "stemm={}\n",
    "pos={}\n",
    "synonym={}\n",
    "related={}\n",
    "gender={}\n",
    "subs={}\n",
    "token={}\n",
    "extra={}\n",
    "wortbildung={}\n",
    "cat={}\n",
    "lemma={}\n",
    "\n",
    "#for title,text in cur:\n",
    "#    if 'Verzeichnis:Deutsch/Wortbildungen/' in title:\n",
    "#        print(title,text)\n",
    "#        break;\n",
    "\n",
    "for title,text in cur:\n",
    "    if text is not None and title.split(':')[0]+':' not in bad_pages and ('{{Sprache|Deutsch}}' in text or '|Deutsch}}' in text.split('===\\n')[0] or '{{Alte Schreibweise' in text.split('===\\n')[0]):\n",
    "        for tag in re.findall('{{[^\\{\\|]{4,}}}',text):\n",
    "            try:\n",
    "                cat[tag]+=1\n",
    "            except:\n",
    "                cat[tag]=1\n",
    "        temp=title.split('==')[0]\n",
    "        token[temp]=None\n",
    "        text=text.split('{{Sprache|Deutsch}}')[-1].split('== '+temp+' ({{')[0]\n",
    "        \n",
    "        #Stamm\n",
    "        for tag in ['{{Grundformverweis','{{Lemmaverweis','{{Alte Schreibweise']:\n",
    "            try:\n",
    "                stemm[temp]=re.search(tag+'[^|]+(.*?)}}', text).group(1).split('|')[1].strip()\n",
    "                break;\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        #Gender\n",
    "        for tag in ['{{Weibliche Wortformen}}','{{Männliche Wortformen}}','{{Sächliche Wortformen}}']:\n",
    "            templist=[]\n",
    "            try:\n",
    "                templist+=re.findall('\\[\\[([^[]+)\\]\\]',re.search(tag+'[^{]+', text).group(0))\n",
    "            except:\n",
    "                pass\n",
    "            if len(templist)>0:\n",
    "                gender[temp]=templist\n",
    "                \n",
    "        #POS   \n",
    "        try:\n",
    "            pos[temp]=re.search('{{Wortart[|]([^|]+)', text).group(1).split('}')[0].split('<!')[0].split('(')[0].strip()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        #Lemmas\n",
    "        try:\n",
    "            synonym[temp]=re.findall('\\[\\[([^[]+)\\]\\]',re.search('{{Synonyme}}[^{]+', text).group(0))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            related[temp]=re.findall('\\[\\[([^[]+)\\]\\]',re.search('{{Sinnverwandte Wörter}}[^{]+', text).group(0))\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            subs[temp]=re.findall('\\[\\[([^[]+)\\]\\]',re.search('{{Unterbegriffe}}[^{]+', text).group(0))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            try:\n",
    "                a=synonym[temp]\n",
    "            except:\n",
    "                a=[]\n",
    "            try:\n",
    "                b=related[temp]\n",
    "            except:\n",
    "                b=[]\n",
    "            try:\n",
    "                c=subs[temp]\n",
    "            except:\n",
    "                c=[]\n",
    "            try:\n",
    "                d=gender[temp]\n",
    "            except:\n",
    "                d=[]\n",
    "                \n",
    "            for word in a+b+c+d:\n",
    "                try:\n",
    "                    lemma[word]+=[temp]\n",
    "                except:\n",
    "                    lemma[word]=[temp]\n",
    "                    token[word]=None\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        \n",
    "        #extra\n",
    "        try:\n",
    "            wortbildung[temp]=[(a+b,c) for a,b,c in re.findall(\"\"\":''\\[\\[([A-Za-zäöüßÄÖÜ]+)\\]\\]:''|(:)|\\[\\[([^[]+)\\]\\]\"\"\",re.sub(\"\"\"\\]\\]e:''\"\"\",\"\"\"]]:''\"\"\",re.search('{{Wortbildungen}}([^{]+)', text).group(1)).replace('\\n',''))]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "conn.close()\n",
    "\n",
    "examples={}\n",
    "for k,v in pos.items():\n",
    "    try:\n",
    "        if len(examples[v])<3:\n",
    "            examples[v]+=[k]\n",
    "    except:\n",
    "        examples[v]=[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versuche unbekannte Wörter (rote Links) mit aufzunehmen\n",
    "unknown={}\n",
    "count=0\n",
    "for tok, wordlist in wortbildung.items():\n",
    "    for tag, word in wordlist:\n",
    "        try:\n",
    "            pos[word]\n",
    "        except:\n",
    "            tag=tag.split('/')[0]\n",
    "            word=word.split('/')[0]\n",
    "            if tag==':':\n",
    "                count+=1\n",
    "                temp=count\n",
    "            if tag!=':' and tag!='':\n",
    "                temp=tag\n",
    "                \n",
    "            temp0=word.split(tok)    \n",
    "            if len(temp0)==2 and len(temp0[0])>0 and len(temp0[1])==0 and pos[tok]=='Verb':\n",
    "                pos[word]=pos[tok]\n",
    "                token[word]=None\n",
    "    \n",
    "            else:\n",
    "                if tag=='' and type(temp) is int:\n",
    "                    if word[0].isupper():\n",
    "                        try:\n",
    "                            if pos[word]=='' or pos[word]==None:\n",
    "                                pos[word]\n",
    "                                token[word]=None\n",
    "                        except:\n",
    "                            pos[word]='Substantiv'\n",
    "                            token[word]=None\n",
    "                    else:\n",
    "                        try:\n",
    "                            unknown[temp]+=[(tok, temp,word)]\n",
    "                        except:\n",
    "                            unknown[temp]=[(tok, temp,word)]\n",
    "\n",
    "                if tag=='' and type(temp) is not int:\n",
    "                    try:\n",
    "                        if pos[word]=='' or pos[word]==None:\n",
    "                            pos[word]=temp\n",
    "                            token[word]=None\n",
    "                    except:\n",
    "                        pos[word]=temp\n",
    "                        token[word]=None\n",
    "    wortbildung[tok]=[word for tag,word in wordlist if word!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "for tok in token:\n",
    "        try:\n",
    "            pos0=pos[tok]\n",
    "        except:\n",
    "            pos0='None'\n",
    "        \n",
    "        try:\n",
    "            stemm0=stemm[tok]\n",
    "        except:\n",
    "            stemm0='None'\n",
    "            \n",
    "        try:\n",
    "            #lemmas={}\n",
    "            #for l in lemma[tok]:\n",
    "            #    try:\n",
    "            #        lemmas.update({k:None for k in lemma[l]})\n",
    "            #    except:\n",
    "            #        pass\n",
    "            lemma0=lemma[tok]\n",
    "        except:\n",
    "            lemma0=['None']\n",
    "            \n",
    "        if lemma0=='':\n",
    "            lemma0=['None']\n",
    "            \n",
    "        line=[tok.split('|')[0],pos0,stemm0,'|'.join(lemma0)]\n",
    "        data.append(line)\n",
    "            \n",
    "df=pd.DataFrame(data, columns=['token','pos','stemm','lemma'])\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db='lexikon' \n",
    "conn = sqlite3.connect(wiktionary_sql)\n",
    "cur = conn.cursor()                                 \n",
    "\n",
    "wildcards = ','.join(['?'] * len(df.columns))              \n",
    "\n",
    "cur.execute(\"drop table if exists %s\" % db)\n",
    "\n",
    "col_str = '\"' + '\",\"'.join(df.columns) + '\"'\n",
    "cur.execute(\"create table %s (%s)\" % (db, col_str))\n",
    "\n",
    "cur.executemany(\"insert into %s values(%s)\" % (db, wildcards), [tuple(x) for x in df.values])\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
